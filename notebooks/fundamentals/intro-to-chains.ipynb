{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Introduction to Chains\n",
    "\n",
    "- Chains are sequences of operations that transform inputs into outputs.\n",
    "- They are the fundamental pattern for connecting components in LangChain.\n",
    "- Chains allow us to focus on composing the flow of logic rather than worrying about the details of execution.\n",
    "- They syntax of chains makes it easy to visualize the flow of data and logic through a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from chain_reaction.config import APIKeys, ModelBehavior, ModelName\n",
    "from chain_reaction.links import init_model_dump_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API keys from .env file\n",
    "api_keys = APIKeys()\n",
    "\n",
    "# Initialize a chat model with your API key\n",
    "chat_model = init_chat_model(\n",
    "    model=ModelName.CLAUDE_HAIKU,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=api_keys.anthropic,\n",
    "    **ModelBehavior.factual().model_dump(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Multi-step workflow\n",
    "\n",
    "1. Prompt LLM to generate some python code\n",
    "2. Analyze the code generated from the first prompt with a second LLM call\n",
    "3. Combine step 1 and 2 to create a complete chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 1. Code generation chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "python_function_writing_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a Python coding assistant adept a writing clean python functions.\"),\n",
    "    (\"user\", \"Write a python function that {task_description}\"),\n",
    "])\n",
    "\n",
    "\n",
    "# Response model\n",
    "class PythonCode(BaseModel):\n",
    "    \"\"\"Python code generation response model.\"\"\"\n",
    "\n",
    "    code: str = Field(description=\"Generated python code.\")\n",
    "    required_libraries: list[str] = Field(description=\"Required libraries for the code to run.\", default_factory=list)\n",
    "    examples: list[str] = Field(description=\"Example usages of the generated code.\", default_factory=list)\n",
    "    test_cases: list[str] = Field(description=\"Test cases to validate the generated code.\", default_factory=list)\n",
    "\n",
    "\n",
    "# Chain\n",
    "code_gen_chain = python_function_writing_template | chat_model.with_structured_output(PythonCode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test chain\n",
    "code_response = code_gen_chain.invoke({\n",
    "    \"task_description\": \"removes duplicates from a list of integers and returns the sorted result.\"\n",
    "})\n",
    "\n",
    "print(\"Generated Code:\")\n",
    "print(code_response.code)\n",
    "\n",
    "print(\"\\nRequired Libraries:\")\n",
    "print(code_response.required_libraries)\n",
    "print(\"\\nExample Usages:\")\n",
    "for example in code_response.examples:\n",
    "    print(f\"- {example}\")\n",
    "\n",
    "print(\"\\nTest Cases:\")\n",
    "for test_case in code_response.test_cases:\n",
    "    print(f\"- {test_case}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 2. Code analysis chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "python_analysis_template = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"You are an expert Python code reviewer versed in modern Python (>= 3.12) best practices.\n",
    "        You make focused, actionable suggestions to improve code quality, documentation, and tests.\"\"\",\n",
    "    ),\n",
    "    (\n",
    "        \"user\",\n",
    "        \"\"\"Please review the following python code and make any recommendations for improvements:\\n{code}\n",
    "        Code requirements: {required_libraries}\n",
    "        Example usages: {examples}\n",
    "        Test cases: {test_cases}\"\"\",\n",
    "    ),\n",
    "])\n",
    "\n",
    "\n",
    "# Response model\n",
    "class PythonCodeAnalysis(BaseModel):\n",
    "    \"\"\"Code analysis response model.\"\"\"\n",
    "\n",
    "    quality_score: int = Field(\n",
    "        description=\"An integer score from 1 to 10 representing the overall quality of the code. 10 is the best.\",\n",
    "        ge=1,\n",
    "        le=10,\n",
    "    )\n",
    "    code_improvements: list[str] = Field(\n",
    "        description=\"List of suggested improvements to the code.\", default_factory=list\n",
    "    )\n",
    "    documentation_improvements: list[str] = Field(\n",
    "        description=\"List of suggested improvements to the documentation.\", default_factory=list\n",
    "    )\n",
    "    test_improvements: list[str] = Field(\n",
    "        description=\"List of suggested improvements to the test cases.\", default_factory=list\n",
    "    )\n",
    "\n",
    "\n",
    "# Chain\n",
    "code_analysis_chain = python_analysis_template | chat_model.with_structured_output(PythonCodeAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test chain\n",
    "analysis_response = code_analysis_chain.invoke(code_response.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 3a. Combine chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_gen_and_analysis_chain = code_gen_chain | init_model_dump_link() | code_analysis_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_response = code_gen_and_analysis_chain.invoke({\n",
    "    \"task_description\": \"removes duplicates from a list of integers and returns the sorted result.\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 3b. Combine chains & preserve outputs\n",
    "\n",
    "- If we want to preserve the output from `code_gen_chain`, we can use `RunnablePassthrough` to store the output\n",
    "- This allows us to maintain context through a chain, rather than passing a single value from start to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_gen_and_analysis_chain = code_gen_chain | {\n",
    "    \"generated_code\": RunnablePassthrough(),  # Preserve the generated code output\n",
    "    \"analysis\": init_model_dump_link() | code_analysis_chain,  # also, pipe to analysis chain\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_response = code_gen_and_analysis_chain.invoke({\n",
    "    \"task_description\": \"removes duplicates from a list of integers and returns the sorted result.\"\n",
    "})\n",
    "\n",
    "full_response.keys()  # dict_keys(['generated_code', 'analysis'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chain-reaction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
