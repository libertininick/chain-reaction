{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Ollama LangChain agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "from chain_reaction.utils import get_last_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Ollama server ``ollama serve`` before running this code.\n",
    "\n",
    "# Initialize the chat model with specific configurations\n",
    "chat_model = ChatOllama(\n",
    "    model=\"qwen3:0.6b\",\n",
    "    temperature=0.5,\n",
    "    base_url=\"http://localhost:11434\",  # Can be changed for remote Ollama instances\n",
    ")\n",
    "\n",
    "# Create chat agent\n",
    "agent = create_agent(\n",
    "    model=chat_model,\n",
    "    system_prompt=\"You are a helpful coding assistant specialized in Python.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question to the agent\n",
    "response = agent.invoke({\"messages\": [HumanMessage(content=\"What is the best thing about Python?\")]})\n",
    "\n",
    "print(get_last_message(response).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask another question to the agent\n",
    "response = agent.invoke({\"messages\": [HumanMessage(content=\"Can you write a hello world program in Python?\")]})\n",
    "\n",
    "print(get_last_message(response).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Ollama CLI Cheat Sheet\n",
    "\n",
    "## Start / Stop Ollama\n",
    "\n",
    "```bash\n",
    "# Start the Ollama server (runs in foreground)\n",
    "ollama serve\n",
    "\n",
    "# Start as background process\n",
    "ollama serve &\n",
    "\n",
    "# Stop Ollama (if running in background)\n",
    "pkill ollama\n",
    "```\n",
    "\n",
    "## Download / Delete Models\n",
    "\n",
    "```bash\n",
    "# Download a model\n",
    "ollama pull llama3.2\n",
    "ollama pull mistral:7b\n",
    "ollama pull codellama:13b\n",
    "\n",
    "# Delete a model\n",
    "ollama rm llama3.2\n",
    "ollama rm mistral:7b\n",
    "```\n",
    "\n",
    "## Check Downloaded Models\n",
    "\n",
    "```bash\n",
    "# List all downloaded models\n",
    "ollama list\n",
    "```\n",
    "\n",
    "## Check Loaded Models\n",
    "\n",
    "```bash\n",
    "# Show currently loaded models (in memory)\n",
    "ollama ps\n",
    "```\n",
    "\n",
    "## Load / Unload Models\n",
    "\n",
    "```bash\n",
    "# Load a model into memory (interactive mode, Ctrl+D to exit)\n",
    "ollama run llama3.2\n",
    "\n",
    "# Load without entering interactive mode\n",
    "echo \"\" | ollama run llama3.2\n",
    "\n",
    "# Unload a model (set keep_alive to 0 via API)\n",
    "curl http://localhost:11434/api/generate -d '{\"model\": \"llama3.2\", \"keep_alive\": 0}'\n",
    "\n",
    "# Unload all models\n",
    "curl http://localhost:11434/api/generate -d '{\"model\": \"\", \"keep_alive\": 0}'\n",
    "```\n",
    "\n",
    "## Check Port\n",
    "\n",
    "```bash\n",
    "# Default port is 11434\n",
    "# Check if Ollama is running and on which port\n",
    "curl http://localhost:11434\n",
    "\n",
    "# Or check with lsof\n",
    "lsof -i -P | grep ollama\n",
    "\n",
    "# Or with ss\n",
    "ss -tlnp | grep ollama\n",
    "```\n",
    "\n",
    "## Additional Commands\n",
    "\n",
    "```bash\n",
    "# Show model info\n",
    "ollama show llama3.2\n",
    "\n",
    "# Copy/rename a model\n",
    "ollama cp llama3.2 my-llama\n",
    "\n",
    "# Create custom model from Modelfile\n",
    "ollama create mymodel -f Modelfile\n",
    "\n",
    "# Check Ollama version\n",
    "ollama --version\n",
    "```\n",
    "\n",
    "## Environment Variables\n",
    "\n",
    "```bash\n",
    "# Change default port\n",
    "OLLAMA_HOST=0.0.0.0:8080 ollama serve\n",
    "\n",
    "# Set models directory\n",
    "OLLAMA_MODELS=/path/to/models ollama serve\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chain-reaction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
