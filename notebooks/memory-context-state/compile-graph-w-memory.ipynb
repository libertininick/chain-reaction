{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ReAct Agent + Memory w/ LangGraph\n",
    "\n",
    "- `act`: let LLM make tool calls\n",
    "- `observe`: pass tool call outputs back to the LLM\n",
    "- `reason`: let the LLM reason about the output of the tool call and decide what to do next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import display as ipy_display\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.messages import AnyMessage, HumanMessage, SystemMessage\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import END, START, MessagesState, StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "from chain_reaction.config import APIKeys, ModelBehavior, ModelName"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Initialize chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = init_chat_model(\n",
    "    model=ModelName.CLAUDE_HAIKU,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=APIKeys().anthropic,\n",
    "    **ModelBehavior.factual().model_dump(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Define tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add to numbers.\n",
    "\n",
    "    Args:\n",
    "        a (float): The first number.\n",
    "        b (float): The second number.\n",
    "\n",
    "    Returns:\n",
    "        float: The sum of the two numbers.\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers.\n",
    "\n",
    "    Args:\n",
    "        a (float): The first number.\n",
    "        b (float): The second number.\n",
    "\n",
    "    Returns:\n",
    "        float: The product of the two numbers.\n",
    "    \"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Bind tools to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model_w_tools = chat_model.bind_tools([add, multiply])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# Define nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool calling node\n",
    "def tool_calling_node(state: MessagesState) -> dict[str, list[AnyMessage]]:\n",
    "    \"\"\"Node that invokes the tool calling LLM.\"\"\"\n",
    "    # System message to guide the model\n",
    "    system_message = SystemMessage(\n",
    "        content=\"You are a helpful assistant that can call tools to perform addition or multiplication.\"\n",
    "    )\n",
    "\n",
    "    # Invoke the model with the system message + current messages\n",
    "    response: AnyMessage = chat_model_w_tools.invoke([system_message, *state[\"messages\"]])\n",
    "\n",
    "    # Return the response\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Tools node\n",
    "tools = ToolNode(tools=[add, multiply])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the graph\n",
    "builder = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# Add nodes to the graph\n",
    "builder.add_node(\"tool_calling_node\", tool_calling_node)\n",
    "builder.add_node(\"tools\", tools)\n",
    "\n",
    "# Define edges\n",
    "builder.add_edge(START, \"tool_calling_node\")\n",
    "builder.add_conditional_edges(\n",
    "    source=\"tool_calling_node\",\n",
    "    path=tools_condition,  # route to \"tools\" or \"__end__\"\n",
    "    path_map={\"tools\": \"tools\", \"__end__\": END},\n",
    ")\n",
    "builder.add_edge(\"tools\", \"tool_calling_node\")  # loop back to tool calling model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Compile graph with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the graph\n",
    "memory = InMemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "# Draw the graph\n",
    "ipy_display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# Invoke graph w/ thread id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define config to track messages\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# First graph invocation\n",
    "print(\"First invocation:\\n\")\n",
    "response = graph.invoke({\"messages\": [HumanMessage(content=\"Add 3 and 5.\")]}, config)\n",
    "for m in response[\"messages\"]:\n",
    "    m.pretty_print()\n",
    "\n",
    "# Second graph invocation\n",
    "print(\"\\nSecond invocation:\\n\")\n",
    "response = graph.invoke({\"messages\": [HumanMessage(content=\"Multiply the output by 2.\")]}, config)\n",
    "for m in response[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chain-reaction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
