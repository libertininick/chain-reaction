{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Managing long conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentState, create_agent\n",
    "from langchain.agents.middleware import SummarizationMiddleware, before_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.messages import AIMessage, HumanMessage, RemoveMessage, ToolMessage\n",
    "from langgraph.runtime import Runtime\n",
    "\n",
    "from chain_reaction.config import APIKeys, ModelBehavior, ModelName\n",
    "from chain_reaction.utils import get_messages\n",
    "\n",
    "api_keys = APIKeys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Summarization\n",
    "\n",
    "Summarization during long conversations serves several critical purposes in agent systems:\n",
    "\n",
    "### 1. Context Window Constraints\n",
    "LLMs have fixed context windows, and as conversations grow, you face a hard ceiling. Without summarization, you either truncate (losing important early context) or hit token limits entirely. Summarization lets you compress historical context while preserving the semantic essence—turning 50k tokens of conversation into 2k tokens of distilled information.\n",
    "\n",
    "### 2. Attention Degradation\n",
    "Even within the context window, LLM attention isn't uniform. Models tend to weight recent tokens more heavily and can \"lose track\" of information buried in the middle of long contexts (the \"lost in the middle\" problem). By summarizing older exchanges, you're essentially promoting that information back into a form the model attends to more reliably.\n",
    "\n",
    "### 3. Output Quality & Coherence\n",
    "Long raw contexts introduce noise. Every tangent, correction, and back-and-forth gets included, making it harder for the model to identify what's actually relevant to the current task. Summarization acts as a relevance filter—the summary naturally emphasizes what mattered and drops what didn't.\n",
    "\n",
    "### 4. Cost & Latency\n",
    "Token count directly impacts API costs and response latency. A 100k token context is expensive and slow. Compressing to 10k tokens with good summarization gives you similar task performance at a fraction of the cost and with faster responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chat model\n",
    "chat_model = init_chat_model(\n",
    "    model=ModelName.CLAUDE_HAIKU,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=api_keys.anthropic,\n",
    "    **ModelBehavior.deterministic().model_dump(),\n",
    ")\n",
    "\n",
    "# Initialize the summarization model\n",
    "summarization_model = init_chat_model(\n",
    "    model=ModelName.CLAUDE_HAIKU,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=api_keys.anthropic,\n",
    "    **ModelBehavior.deterministic().model_dump(),\n",
    ")\n",
    "\n",
    "# Define the summarization middleware\n",
    "summarization_middleware = SummarizationMiddleware(\n",
    "    model=summarization_model,\n",
    "    trigger=[\n",
    "        (\"tokens\", 100),  # trigger summarization when 100 tokens are exceeded\n",
    "        (\"messages\", 3),  # or trigger summarization when 3 messages are exceeded\n",
    "    ],\n",
    "    keep=(\"messages\", 1),  # Keep only the last message in the conversation history (+ summary)\n",
    ")\n",
    "\n",
    "# Create an agent with summarization middleware\n",
    "agent = create_agent(\n",
    "    model=chat_model,\n",
    "    middleware=[summarization_middleware],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the agent with a \"long\" conversation\n",
    "response = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Hello!\"),\n",
    "            AIMessage(content=\"Hi there! How can I assist you today?\"),\n",
    "            HumanMessage(content=\"Can you tell me a joke?\"),\n",
    "            AIMessage(content=\"Sure! Why did the scarecrow win an award? Because he was outstanding in his field!\"),\n",
    "            HumanMessage(content=\"Haha, that's a good one! Can you tell me another joke?\"),\n",
    "            AIMessage(content=\"Of course! Why don't scientists trust atoms? Because they make up everything!\"),\n",
    "            HumanMessage(content=\"LOL! You're on a roll. One more, please?\"),\n",
    "            AIMessage(content=\"Alright! Why did the bicycle fall over? Because it was two-tired!\"),\n",
    "            HumanMessage(content=\"These are great! Thanks for the laughs. Can you tell me a fun fact now?\"),\n",
    "        ]\n",
    "    },\n",
    ")\n",
    "\n",
    "# Extract messages from the response\n",
    "messages = get_messages(response)\n",
    "\n",
    "# Assert there are only 3 messages (summary, last message from sequence, AI response to last message)\n",
    "assert len(messages) == 3, f\"Expected 3 messages, got {len(messages)}\"  # noqa: S101\n",
    "\n",
    "# Print summary message\n",
    "print(\"Summary Message:\")\n",
    "print(messages[0].content)\n",
    "\n",
    "# Print AI's response to the last user message\n",
    "print(\"\\nAI's Response to Last User Message:\")\n",
    "print(messages[2].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Trim messages\n",
    "\n",
    "Rather than summarizing a long sequence of messages, we can use a custom middleware function to filter out messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@before_agent\n",
    "def remove_tool_messages(state: AgentState, runtime: Runtime) -> dict[str, list[RemoveMessage]]:  # noqa: ARG001\n",
    "    \"\"\"Remove ToolMessages from the message history before sending to agent.\"\"\"\n",
    "    # Get all messages from the state\n",
    "    messages = get_messages(state)\n",
    "\n",
    "    # Remove ToolMessages\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            RemoveMessage(id=msg.id) for msg in messages if isinstance(msg, ToolMessage) and msg.id is not None\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "# Create an agent with message trimming middleware\n",
    "agent = create_agent(\n",
    "    model=chat_model,\n",
    "    middleware=[remove_tool_messages],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the agent with a \"long\" conversation interlaced with ToolMessages\n",
    "messages = [\n",
    "    HumanMessage(content=\"Hello!\"),\n",
    "    AIMessage(content=\"Hi there! How can I assist you today?\"),\n",
    "    HumanMessage(content=\"Can you tell me a joke?\"),\n",
    "    ToolMessage(content=\"Invoking joke tool...\", tool_call_id=\"1\"),\n",
    "    AIMessage(content=\"Sure! Why did the scarecrow win an award? Because he was outstanding in his field!\"),\n",
    "    HumanMessage(content=\"Haha, that's a good one! Can you tell me another joke?\"),\n",
    "    ToolMessage(content=\"Invoking feedback tool...\", tool_call_id=\"2\"),\n",
    "    ToolMessage(content=\"Invoking joke tool...\", tool_call_id=\"3\"),\n",
    "    AIMessage(content=\"Of course! Why don't scientists trust atoms? Because they make up everything!\"),\n",
    "    HumanMessage(content=\"LOL! You're on a roll. One more, please?\"),\n",
    "    ToolMessage(content=\"Invoking feedback tool...\", tool_call_id=\"4\"),\n",
    "    ToolMessage(content=\"Invoking joke tool...\", tool_call_id=\"5\"),\n",
    "    AIMessage(content=\"Alright! Why did the bicycle fall over? Because it was two-tired!\"),\n",
    "    HumanMessage(content=\"These are great! Thanks for the laughs. Can you tell me a fun fact now?\"),\n",
    "]\n",
    "response = agent.invoke(\n",
    "    {\"messages\": messages},\n",
    ")\n",
    "\n",
    "# Extract messages from the response\n",
    "messages = get_messages(response)\n",
    "\n",
    "# Check that tool messages have been removed\n",
    "assert all(not isinstance(msg, ToolMessage) for msg in messages), \"ToolMessages not removed\"  # noqa: S101"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chain-reaction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
