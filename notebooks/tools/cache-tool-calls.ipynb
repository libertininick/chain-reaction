{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import datetime\n",
    "from time import sleep\n",
    "from typing import Any\n",
    "\n",
    "from diskcache import Cache\n",
    "from langchain.agents import create_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.messages import HumanMessage\n",
    "from langchain.tools import tool\n",
    "from pydantic import BaseModel, Field\n",
    "from tavily import TavilyClient\n",
    "\n",
    "from chain_reaction.caching import cache_calls\n",
    "from chain_reaction.config import APIKeys, ModelBehavior, ModelName\n",
    "from chain_reaction.utils import get_structured_response\n",
    "\n",
    "# Load API keys\n",
    "api_keys = APIKeys()\n",
    "\n",
    "# Define temporary disk cache for caching tool calls\n",
    "cache = Cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Simple example with mock tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "@cache_calls(cache=cache)\n",
    "def tool_with_cache(x: float) -> float:\n",
    "    \"\"\"A tool that squares a number with simulated delay.\"\"\"\n",
    "    sleep(2)  # Simulate a time-consuming computation\n",
    "    return x * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_with_cache.get_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the tool for first time (will take ~2 seconds)\n",
    "tool_with_cache.invoke({\"x\": 3.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the tool to see caching in action (will be instantaneous)\n",
    "tool_with_cache.invoke({\"x\": 3.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the value directly from the cache\n",
    "cache.get(key=\"2a613a05fab2d2cc4e56aea0e7e26871\", tag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Cached web searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Tavily client\n",
    "tavily_client = TavilyClient(api_key=api_keys.tavily.get_secret_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tool for searching the web using Tavily with caching\n",
    "@tool\n",
    "@cache_calls(cache=cache)\n",
    "def search_web(query: str) -> dict[str, Any]:\n",
    "    \"\"\"Performs a web search using Tavily.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, Any]: The search results.\n",
    "    \"\"\"\n",
    "    return tavily_client.search(query=query)\n",
    "\n",
    "\n",
    "search_web.invoke({\"query\": \"When will it snow next in Centennial, CO?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search again, and get from cache\n",
    "search_web.invoke({\"query\": \"When will it snow next in Centennial, CO?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Web search agent with caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a chat model\n",
    "chat_model = init_chat_model(\n",
    "    model=ModelName.CLAUDE_HAIKU,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=api_keys.anthropic,\n",
    "    **ModelBehavior.factual().model_dump(),\n",
    ")\n",
    "\n",
    "\n",
    "# Create a response model\n",
    "class WeatherResult(BaseModel):\n",
    "    \"\"\"Response model for weather results.\"\"\"\n",
    "\n",
    "    forecast_date: datetime.date | None = Field(description=\"The date of the weather forecast.\")\n",
    "    chance_of_snow: float | None = Field(description=\"The chance of snow in the specified location.\", ge=0, le=100)\n",
    "    amount_of_snow: float | None = Field(description=\"The expected amount of snow in inches.\", ge=0)\n",
    "    temperature: float | None = Field(description=\"The expected temperature in Fahrenheit.\")\n",
    "    snow_start: int | None = Field(\n",
    "        description=\"The hour when snow is expected to start (24-hour format). None if no snow is expected.\",\n",
    "        ge=0,\n",
    "        le=23,\n",
    "    )\n",
    "    snow_end: int | None = Field(\n",
    "        description=\"The hour when snow is expected to end (24-hour format). None if no snow is expected.\", ge=0, le=23\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def get_result(cls, response: dict[str, Any]) -> WeatherResult:\n",
    "        \"\"\"Parse the weather result from the model response.\"\"\"\n",
    "        result = get_structured_response(model=cls, response=response)\n",
    "        if result is None:\n",
    "            raise ValueError(\"Failed to parse weather result from response.\")\n",
    "        return result\n",
    "\n",
    "\n",
    "# Initialize an agent using the chat model & tools\n",
    "agent = create_agent(\n",
    "    model=init_chat_model(\n",
    "        model=ModelName.CLAUDE_HAIKU,\n",
    "        timeout=None,\n",
    "        max_retries=2,\n",
    "        api_key=api_keys.anthropic,\n",
    "        **ModelBehavior.deterministic().model_dump(),\n",
    "    ),\n",
    "    tools=[search_web],\n",
    "    system_prompt=\"\"\"\n",
    "    You're a helpful assistant that can search the web for weather information.\n",
    "    Use the provided web search tools to answer user questions accurately.\n",
    "    ONLY use the tools when necessary to find up-to-date weather information (2-3 searches max per conversation).\n",
    "    If you can't find information on a specific field, populate it with None.\n",
    "    But try to find as much information as possible.\n",
    "    \"\"\",\n",
    "    response_format=WeatherResult,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the agent to get weather information\n",
    "response = agent.invoke(input={\"messages\": [HumanMessage(content=\"When will it snow next in Centennial, CO?\")]})\n",
    "WeatherResult.get_result(response=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke again to see if the agent uses any of the same tool calls\n",
    "response = agent.invoke(input={\"messages\": [HumanMessage(content=\"When will it snow next in Centennial, CO?\")]})\n",
    "WeatherResult.get_result(response=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chain-reaction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
