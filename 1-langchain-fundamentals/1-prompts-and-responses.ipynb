{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Prompts & Responses\n",
    "\n",
    "1. Generate reusable prompt templates\n",
    "2. Parse responses using pydantic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from chain_reaction.config import APIKeys, ModelName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API keys from .env file\n",
    "api_keys = APIKeys()\n",
    "\n",
    "# Initialize a chat model with your API key\n",
    "chat_model = init_chat_model(\n",
    "    model=ModelName.CLAUDE_HAIKU,\n",
    "    temperature=0,\n",
    "    max_tokens=1024,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=api_keys.anthropic,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# 1. Reusable prompt template\n",
    "\n",
    "Templates offer:\n",
    "- **Consistency**: Standardize prompts across your application.\n",
    "- **Maintainability**: Allow you to change the prompt structure in one place instead of throughout your codebase.\n",
    "- **Modularity**: We can compose multiple message templates together to create reusable templates.\n",
    "- **Validation**: User input can be validated before injecting into the prompt template\n",
    "- **Testability**: We can test prompt creation independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple chat prompt template with input variables\n",
    "class PoemInputs(BaseModel):\n",
    "    \"\"\"Poem prompt input variables.\n",
    "\n",
    "    Attributes:\n",
    "        topic (str): The topic of the poem.\n",
    "        funny_or_sad (Literal[\"funny\", \"sad\"]): Whether the poem should be funny or sad. Default is 'funny'.\n",
    "        audience (str): The intended audience for the poem. Default is 'general'.\n",
    "    \"\"\"\n",
    "\n",
    "    topic: str = Field(description=\"The topic of the poem\")\n",
    "    funny_or_sad: Literal[\"funny\", \"sad\"] = Field(\n",
    "        default=\"funny\", description=\"Whether the poem should be funny or sad. Default is 'funny'.\"\n",
    "    )\n",
    "    audience: str = Field(default=\"general\", description=\"The intended audience for the poem.\")\n",
    "\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    template=\"Please write me a short poem about {topic}. This poem should be {funny_or_sad} for {audience}.\",\n",
    ")\n",
    "print(\"Prompt template created successfully.\", \"\\n\", prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get message prompt from the template\n",
    "message_prompt = prompt_template.format_messages(\n",
    "    topic=\"getting dressed in the morning\", funny_or_sad=\"funny\", audience=\"children\"\n",
    ")\n",
    "print(\"Message prompt formatted successfully.\", \"\\n\", message_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a different message prompt from the template using PoemVariables model\n",
    "prompt_variables = PoemInputs(topic=\"eating soup\", funny_or_sad=\"funny\", audience=\"6 year old boy\")\n",
    "message_prompt = prompt_template.format_messages(**prompt_variables.model_dump(mode=\"json\"))\n",
    "print(\"Message prompt formatted successfully.\", \"\\n\", message_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt the model and get the response\n",
    "response = chat_model.invoke(message_prompt)\n",
    "\n",
    "print(\"Poem response:\", \"\\n\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to create an invalid message prompt from the template\n",
    "prompt_variables = PoemInputs(topic=\"eating soup\", funny_or_sad=\"warm and cozy\")  # This should raise a validation error\n",
    "message_prompt = prompt_template.format_messages(**prompt_variables.model_dump(mode=\"json\"))\n",
    "print(\"Message prompt formatted successfully.\", \"\\n\", message_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a template from messages\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a master poem composer highly skilled a writing poems that elicit the perfect combination of emotion.\",\n",
    "    ),\n",
    "    (\"user\", \"Please write me a short poem about {topic}. This poem should be {funny_or_sad} for {audience}.\"),\n",
    "])\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# 2. Enforce structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the response model for structured output\n",
    "class PoemOutputs(BaseModel):\n",
    "    \"\"\"Poem response output variables.\n",
    "\n",
    "    Attributes:\n",
    "        title (str): The title of the poem.\n",
    "        content (str): The content of the poem.\n",
    "        voice (str | None): Recommended voice for reading the poem aloud. Optional.\n",
    "    \"\"\"\n",
    "\n",
    "    title: str = Field(description=\"The title of the poem\")\n",
    "    content: str = Field(description=\"The content of the poem.\")\n",
    "    voice: str | None = Field(default=None, description=\"Recommended voice for reading the poem aloud. Optional.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Option A: `PydanticOutputParser` & format instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "# Create a Pydantic output parser for the response model\n",
    "response_parser = PydanticOutputParser(pydantic_object=PoemOutputs)\n",
    "\n",
    "# Create a prompt template with response format instructions\n",
    "prompt_template_w_format = ChatPromptTemplate.from_template(\n",
    "    template=(\n",
    "        \"Please write me a short poem about {topic}. This poem should be {funny_or_sad} for {audience}.\"\n",
    "        \"\\n\\n{format_instructions}\"\n",
    "    ),\n",
    "    partial_variables={\"format_instructions\": response_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain combining the prompt, chat model, and response parser\n",
    "chain = prompt_template_w_format | chat_model | response_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the chain with prompt variables\n",
    "prompt_variables = PoemInputs(topic=\"eating soup\", funny_or_sad=\"funny\", audience=\"6 year old boy\")\n",
    "response: PoemOutputs = chain.invoke(prompt_variables.model_dump(mode=\"json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the structured response\n",
    "print(\"Poem Title:\", response.title)\n",
    "print(\"\\nPoem Content:\", response.content)\n",
    "print(\"\\nRecommended Voice:\", response.voice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Option B: New model `with_structured_output`\n",
    "\n",
    " I'd strongly recommend using `.with_structured_output()` instead of `PydanticOutputParser` for Claude models. It's more reliable because it uses Claude's native tool-calling rather than trying to parse JSON from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the chat model to enforce structured output\n",
    "structured_model = chat_model.with_structured_output(PoemOutputs)\n",
    "\n",
    "# Create a chain combining the (original) prompt template and the structured model\n",
    "chain = prompt_template | structured_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the chain with prompt variables\n",
    "response: PoemOutputs = chain.invoke(PoemInputs(topic=\"playing Lego with my sister\").model_dump(mode=\"json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the structured response\n",
    "print(\"Poem Title:\", response.title)\n",
    "print(\"\\nPoem Content:\", response.content)\n",
    "print(\"\\nRecommended Voice:\", response.voice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# Example: Review parser\n",
    "\n",
    "Build a parsers for analyzing book reviews to determine if the review is:\n",
    "\n",
    "1. About the book in question\n",
    "2. Overall {Negative, Positive, or Neural}\n",
    "3. Pros of the book\n",
    "4. Cons of the book\n",
    "5. Other books to consider\n",
    "\n",
    "\n",
    "Here we will use a response model with rich field descriptions help the model understand exactly what we want.\n",
    "- Field descriptions can be used to provide detailed instructions to the LLM with structure\n",
    "  \n",
    "  ```python\n",
    "  quality_review: bool = Field(\n",
    "        description=(\n",
    "            \"Whether the review is a quality review that provides useful information about the book's content. \"\n",
    "            \"Instead of just focusing on extraneous details, like shipping speed or paper quality.\"\n",
    "        )\n",
    "    )\n",
    "  ```\n",
    "- You can add examples in descriptions if needed for clarity\n",
    "- Use the response model's class docstring for overall context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookReviewInputs(BaseModel):\n",
    "    \"\"\"Book review prompt input variables.\n",
    "\n",
    "    Attributes:\n",
    "        book_title (str): The title of the book being reviewed.\n",
    "        book_overview (str): A brief overview of the book.\n",
    "        review_title (str): The title of the book review.\n",
    "        review_text (str): The text of the book review.\n",
    "    \"\"\"\n",
    "\n",
    "    book_title: str = Field(description=\"The title of the book being reviewed.\")\n",
    "    book_overview: str = Field(description=\"A brief overview of the book from the publisher.\")\n",
    "    review_title: str = Field(description=\"The title of the book review.\")\n",
    "    review_text: str = Field(description=\"The text of the book review.\")\n",
    "\n",
    "\n",
    "class BookReviewOutputs(BaseModel):\n",
    "    \"\"\"Book review analysis output variables.\n",
    "\n",
    "    Attributes:\n",
    "        is_about_book (bool): Whether the review is about the book in question.\n",
    "        quality_review (bool): Whether the review is a quality review.\n",
    "        overall_sentiment (Literal[\"Negative\", \"Positive\", \"Neutral\"]): The overall sentiment of the review.\n",
    "        pros (str): The pros of the book mentioned in the review.\n",
    "        cons (str): The cons of the book mentioned in the review.\n",
    "        other_books_to_consider (str): Other books to consider mentioned in the review.\n",
    "    \"\"\"\n",
    "\n",
    "    is_about_book: bool = Field(description=\"Whether the review is about the book in question.\")\n",
    "    quality_review: bool = Field(\n",
    "        description=(\n",
    "            \"Whether the review is a quality review that provides useful information about the book's content. \"\n",
    "            \"Instead of just focusing on extraneous details, like shipping speed or paper quality.\"\n",
    "        )\n",
    "    )\n",
    "    overall_sentiment: Literal[\"Negative\", \"Positive\", \"Neutral\"] = Field(\n",
    "        description=\"The overall sentiment of the review.\"\n",
    "    )\n",
    "    pros: list[str] = Field(description=\"The pros of the book mentioned in the review.\")\n",
    "    cons: list[str] = Field(description=\"The cons of the book mentioned in the review.\")\n",
    "    other_books_to_consider: list[str] = Field(description=\"Other books to consider mentioned in the review.\")\n",
    "\n",
    "\n",
    "book_review_template = ChatPromptTemplate.from_template(\n",
    "    template=(\n",
    "        \"You are an expert book review analyst. \"\n",
    "        \"Given the book details, analyze the review relative to the book overview. \"\n",
    "        \"Extract the requested information in a structured format.\\n\\n\"\n",
    "        \"Analyze this book review:\\n\"\n",
    "        \"Book Title: {book_title}\\n\"\n",
    "        \"Book Overview: {book_overview}\\n\"\n",
    "        \"Review Title: {review_title}\\n\"\n",
    "        \"Review Text: {review_text}\\n\\n\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Wrap the chat model to enforce structured output\n",
    "chat_model.max_tokens = 4096\n",
    "book_reviewer_model = chat_model.with_structured_output(BookReviewOutputs)\n",
    "\n",
    "# Create a chain combining the prompt template and the book reviewer model\n",
    "book_review_chain = book_review_template | book_reviewer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_title = (\n",
    "    \"Generative AI with LangChain: \"\n",
    "    \"Build production-ready LLM applications and advanced agents using Python, LangChain, and LangGraph\"\n",
    ")\n",
    "book_overview = \"\"\"\n",
    "This second edition tackles the biggest challenge facing companies in AI today: moving from prototypes to production.\n",
    "Fully updated to reflect the latest developments in the LangChain ecosystem, it captures how modern AI systems are\n",
    "developed, deployed, and scaled in enterprise environments. This edition places a strong focus on multi-agent\n",
    "architectures, robust LangGraph workflows, and advanced retrieval-augmented generation (RAG) pipelines.\n",
    "\n",
    "You'll explore design patterns for building agentic systems, with practical implementations of multi-agent setups\n",
    "for complex tasks. The book guides you through reasoning techniques such as Tree-of -Thoughts, structured generation,\n",
    "and agent handoffs—complete with error handling examples. Expanded chapters on testing, evaluation, and deployment\n",
    "address the demands of modern LLM applications, showing you how to design secure, compliant AI systems with\n",
    "built-in safeguards and responsible development principles. This edition also expands RAG coverage with guidance on\n",
    "hybrid search, re-ranking, and fact-checking pipelines to enhance output accuracy.\n",
    "\n",
    "What you will learn\n",
    "Design and implement multi-agent systems using LangGraph\n",
    "Implement testing strategies that identify issues before deployment\n",
    "Deploy observability and monitoring solutions for production environments\n",
    "Build agentic RAG systems with re-ranking capabilities\n",
    "Architect scalable, production-ready AI agents using LangGraph and MCP\n",
    "Work with the latest LLMs and providers like Google Gemini, Anthropic, Mistral, DeepSeek, and OpenAI's o3-mini\n",
    "Design secure, compliant AI systems aligned with modern ethical practices\n",
    "Who this book is for\n",
    "This book is for developers, researchers, and anyone looking to learn more about LangChain and LangGraph.\n",
    "With a strong emphasis on enterprise deployment patterns, it's especially valuable for teams implementing LLM solutions\n",
    "at scale. While the first edition focused on individual developers, this updated edition expands its reach to support\n",
    "engineering teams and decision-makers working on enterprise-scale LLM strategies. A basic understanding of Python is\n",
    "required, and familiarity with machine learning will help you get the most out of this book.\n",
    "\n",
    "Table of Contents\n",
    "The Rise of Generative AI: From Language Models to Agents\n",
    "First Steps with LangChain\n",
    "Building Workflows with LangGraph\n",
    "Building Intelligent RAG Systems with LangChain\n",
    "Building Intelligent Agents\n",
    "Advanced Applications and Multi-Agent Systems\n",
    "Software Development and Data Analysis Agents\n",
    "Evaluation and Testing\n",
    "Observability and Production Deployment\n",
    "The Future of LLM Applications\n",
    "\"\"\"\n",
    "\n",
    "# Define some example reviews\n",
    "reviews = [\n",
    "    (\n",
    "        \"Modern LLM Practical Guide\",\n",
    "        \"Generative AI with LangChain is a comprehensive, technically rich, and highly practical guide for developers \"\n",
    "        \"and architects building LLM applications. it elaborates LangGraph—a powerful orchestration framework built on \"\n",
    "        \"top of LangChain—and teaches how to use it to build structured, multi-agent workflows capable of reasoning, \"\n",
    "        \"coordination, and error recovery. One of its greatest strengths lies in demystifying the journey from \"\n",
    "        \"prototype to production by introducing scalable design patterns that incorporate advanced techniques like \"\n",
    "        \"retrieval-augmented generation (RAG), hybrid search, re-ranking, and fact-checking. Readers are guided \"\n",
    "        \"through intelligent agent construction using LangGraph's node and edge abstractions, bringing determinism \"\n",
    "        \"and modularity to systems that are otherwise probabilistic and fragile. A major focus of the book is showing \"\n",
    "        \"how to architect robust workflows that go beyond one-shot queries or simple chatbots. It includes examples \"\n",
    "        \"of specialized agents for tasks such as data analysis and software development—use cases that are \"\n",
    "        \"increasingly being adopted across industries. Moreover, the book doesn't shy away from difficult topics \"\n",
    "        \"like observability, monitoring, and testing in dynamic LLM environments. Concepts such as latency tracing, \"\n",
    "        \"model output evaluation, and prompt governance are treated as essential, not optional. For teams concerned \"\n",
    "        \"with responsible AI practices, there's also guidance on embedding security, compliance, and ethical \"\n",
    "        \"development into the system design from the outset. While it assumes basic Python skills, it remains \"\n",
    "        \"friendly to intermediate readers, and the advanced topics will be particularly valuable for experienced \"\n",
    "        \"teams working on critical production systems. Overall, this second edition serves as a field guide for \"\n",
    "        \"anyone serious about building intelligent, scalable, and trustworthy generative AI applications. It is an \"\n",
    "        \"essential resource for moving from lab experiments to reliable deployments, offering practical blueprints \"\n",
    "        \"for architecting AI agents that can reason, interact, and perform in real-world enterprise environments. \"\n",
    "        \"Whether you're working solo or as part of a cross-functional platform team, The inclusion of multi-agent \"\n",
    "        \"coordination patterns—where multiple agents can specialize, delegate, and hand off tasks—offers a glimpse \"\n",
    "        \"into what the future of enterprise AI may look like. this book will give you the patterns, tools, and \"\n",
    "        \"confidence to deliver LLM-powered solutions that are not only smart, but stable and secure.\",\n",
    "    ),\n",
    "    (\n",
    "        \"Great Resource for those working for Lang Chain\",\n",
    "        \"I'm working on building some AI products for work. This has been a great resource to learn how to properly \"\n",
    "        \"leverage LangChain and LangGraph when building AI products using GenAI.\",\n",
    "    ),\n",
    "    (\"Felt a bit deceived!\", \"Says “In color” but it has no color!\"),\n",
    "]\n",
    "\n",
    "# Create inputs for the book review chain\n",
    "inputs = [\n",
    "    BookReviewInputs(\n",
    "        book_title=book_title, book_overview=book_overview, review_title=review_title, review_text=review_text\n",
    "    )\n",
    "    for review_title, review_text in reviews\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the book reviews\n",
    "analysis: list[BookReviewOutputs] = book_review_chain.batch([ii.model_dump(mode=\"json\") for ii in inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the analysis results\n",
    "for i, result in enumerate(analysis):\n",
    "    print(f\"\\nAnalysis for Review {i + 1}:\")\n",
    "    print(result.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chain-reaction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
