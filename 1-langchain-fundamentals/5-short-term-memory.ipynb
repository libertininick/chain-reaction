{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Understanding and managing LLM short-term memory\n",
    "\n",
    "- Large language models are stateless; each invocation is independent\n",
    "  - Models do not remember what was sent in the previous message\n",
    "  - Models do not \"learn\" from a conversation while they are in inference mode\n",
    "- But, chat conversations appear to have memory because conversation history is provided as \"context\"\n",
    "- Deciding what to provide as historical context is the \"art\" of LLM memory management\n",
    "- `LangChain` provides different kinds of \"memory\" to store and accumulate the context of a conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.messages import HumanMessage\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "from chain_reaction.config import APIKeys, ModelName\n",
    "from chain_reaction.memory import ChatSessionHistoryManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API keys from .env file\n",
    "api_keys = APIKeys()\n",
    "\n",
    "# Initialize a chat model with your API key\n",
    "chat_model = init_chat_model(\n",
    "    model=ModelName.CLAUDE_HAIKU,\n",
    "    temperature=0,\n",
    "    max_tokens=1024,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=api_keys.anthropic,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# A forgetful chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize prompt template with system message and placeholders for chat history\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are a friendly and helpful chat model. \"\n",
    "    \"The most important thing is to always remember the user's name and use it in the conversation. \"\n",
    "    \"If you don't know the user's name, ask for it politely.\"\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    system_prompt,\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{user_input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chat session history manager with a limit of 0 message (forgetful)\n",
    "chat_session_manager = ChatSessionHistoryManager(max_messages=0)\n",
    "session_config = chat_session_manager.create_session_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the runnable pipeline with message history\n",
    "pipeline = prompt_template | chat_model\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=chat_session_manager.get_chat_history,\n",
    "    input_messages_key=\"user_input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 1: User provides their name and asks for a joke\n",
    "response = pipeline_with_history.invoke(\n",
    "    {\"user_input\": \"My name is Nick, tell me a joke about pydantic.\"},\n",
    "    config=session_config,\n",
    ")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 2: User reacts to the joke and asks to be reminded of their name\n",
    "response2 = pipeline_with_history.invoke(\n",
    "    {\"user_input\": \"Wow, that's funny! Can you remind me what my name is?\"},\n",
    "    config=session_config,\n",
    ")\n",
    "print(response2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Nick\" in response2.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 3: User gives their name again and asks for a joke\n",
    "response3 = pipeline_with_history.invoke(\n",
    "    {\n",
    "        \"user_input\": (\n",
    "            \"No problem, my name is Nick, please don't forget it this time! Tell me a joke about short-term memory.\"\n",
    "        )\n",
    "    },\n",
    "    config=session_config,\n",
    ")\n",
    "print(response3.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 4: User checks if the model remembers their name\n",
    "response4 = pipeline_with_history.invoke(\n",
    "    {\"user_input\": \"Good one! Ok, this should be easy now, what's my name?\"},\n",
    "    config=session_config,\n",
    ")\n",
    "print(response4.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Nick\" in response4.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# Agent with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an agent with memory\n",
    "agent = create_agent(\n",
    "    model=chat_model,\n",
    "    checkpointer=InMemorySaver(),\n",
    "    system_prompt=system_prompt.prompt.template,\n",
    ")\n",
    "\n",
    "# Initialize configuration for a new chat session\n",
    "config = {\"configurable\": {\"thread_id\": 1}}\n",
    "\n",
    "# Prompt agent\n",
    "response = agent.invoke(\n",
    "    input={\"messages\": [HumanMessage(\"My name is Nick, tell me a joke about pydantic.\")]},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_2 = agent.invoke(\n",
    "    input={\"messages\": [HumanMessage(\"Wow, that's funny! Can you remind me what my name is?\")]},\n",
    "    config=config,\n",
    ")\n",
    "response_2[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chain-reaction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
